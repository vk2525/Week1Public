{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Problem Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hepatitis B (HEP B) is a liver infection caused by the hepatitis B virus (HBV). The infection causes inflammation of the liver and if not properly treated, the virus can lead to liver disease such as cirrhosis or liver cancer. \n",
    "HEP B is the most primary cause of liver cancer, the 2nd leading cause of cancer deaths in the world, therfore making it a major global health problem. HEP B is up to 100 times more infectious than the HIV/AIDS virus. Two billion people (1 in 3) have been infected and more than 292 million people are living with a chronic hepatitis B infection. Although HEP B is treatable and preventable about 884,000 people die each year.\n",
    "\n",
    "The virus is transmitted through the blood and infected bodily fluids. It can be passed to others through direct contact with blood, unprotected sex, use of illegal drugs, unsterilized or contaminated needles, and from an infected woman to her newborn during pregnancy or childbirth. Most people do not show symptoms and the only way to know you are infected is by getting tested.\n",
    "\n",
    "![hepb](https://images.onhealth.com/images/slideshow/hepatitis-s1-liver-hepatitis-virus.jpg)\n",
    "\n",
    "**Goal**: Use the NHANES data set to predict whether a patient has HEP B or not. We want to determine which attributes are the most meaningful to the predictive models. We want to create a balanced model that can predict with a high sensitivity and high specificity while using the **least amount of features**. Essentially is there a way to identify the population of those infected without testing them? \n",
    "\n",
    "Source: https://www.hepb.org/what-is-hepatitis-b/what-is-hepb/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# National Health and Nutrition Examination Survey NHANES \n",
    "To investigate our research problem we will be using the NHANES database. NHANES is a program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey is unique in that it combines interviews and physical examinations. The survey examines a nationally representative sample of about 5,000 persons each year. These persons are located in counties across the country, 15 of which are visited each year. The NHANES interview includes demographic, socioeconomic, dietary, and health-related questions. The examination component consists of medical, dental, and physiological measurements, as well as laboratory tests administered by highly trained medical personnel.\n",
    "\n",
    "Source: Centers for Disease Control and Prevention (CDC). National Center for Health Statistics (NCHS). National Health and Nutrition Examination Survey Data. Hyattsville, MD: U.S. Department of Health and Human Services, Centers for Disease Control and Prevention, [2019][https://www.cdc.gov/nchs/nhanes/about_nhanes.htm#data]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Below are some general steps to begin analyzing this problem. Apply the new material you learned in class and have fun! (:\n",
    "\n",
    "1. Import the data  \n",
    "2. Decide what variables are most relevant  \n",
    "3. Summary statistics of the data  \n",
    "4. Data Cleaning (Important!) Note this may a tedious process  \n",
    "a. Missing data  \n",
    "b. Transform/Normalize data  \n",
    "4. Data Visualization  \n",
    "5. Data analysis  \n",
    "a. Create dummy variables  \n",
    "b. Create training and test sets  \n",
    "c. Statistical methodology  \n",
    "6. Scoring metrics  \n",
    "confusion matrix, roc curve  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "#import os\n",
    "\n",
    "#os.chdir(\"./Week1Public\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "Read in the data set and look at the first ten lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "# Write your code here\n",
    "\n",
    "dataset = pd.read_csv(\"NHANES_comp.csv\") \n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unnecessary variables (don't worry about this)\n",
    "dataset = dataset[dataset.columns.drop(list(dataset.filter(regex='WT')))]\n",
    "dataset = dataset[dataset.columns.drop(list(dataset.filter(regex='SDM')))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, looks like the data loaded in properly. Let's continue by looking at variables that may be predictive of hepatitis B. For beginners, I would suggest conducting a literature review on previous research of hepatitis B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Features of Interest\n",
    "Once you have selected some variables in the NHANES data set, make a smaller subset as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subset of the data you want to analyze\n",
    "# Write your code here\n",
    "mydat = dataset[['LBXHBC', 'RIDAGEYR', 'RIDRETH1', 'RIAGENDR']] # create a subset of the selected features you want to investigate\n",
    "mydat = mydat.rename(index = str, columns = {\"LBXHBC\": \"HEPB\", \"RIDAGEYR\": \"Age\",\"RIDRETH1\":\"Ethn\", \"RIAGENDR\":\"Sex\" }) #renaming variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the goal is to create a balanced model that can predict with a high sensitivity and high specificity while using the **least amount of features**. Next, we will look at some summary statistics of the variables you chose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View summary statistics\n",
    "Some useful functions in pandas are describe() and info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info about the data including the index dtype and column dtypes, non-null values and memory usage\n",
    "mydat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the data types are float64, int64 or objects--if there are columns that are obviously numeric like Age but show as objects (or vice versa), we need to change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify columns by what type of data they hold --first numeric columns\n",
    "numeric_columns = list([\"Age\"])\n",
    "\n",
    "# categorical columns are everything else \n",
    "categorical_columns = list(set(mydat.columns) - set(numeric_columns))\n",
    "\n",
    "\n",
    "# convert numeric columns from strings to numbers\n",
    "mydat[numeric_columns] = mydat[numeric_columns].apply(pd.to_numeric)\n",
    "\n",
    "# print statement for sanity check\n",
    "print('Numerical Columns: ', numeric_columns)\n",
    "print('Categorical Columns: ', categorical_columns)\n",
    "\n",
    "mydat.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the counts for the columns are different therefore I will remove the rows with NAs (you should deal with missing data https://scikit-learn.org/stable/modules/impute.html#impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NAs\n",
    "\n",
    "mydat = mydat.dropna(axis='rows')\n",
    "mydat.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will convert HEP B and Sex into indicator variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert HEPB and Sex categorical column to indicator (0,1) variables (binary)\n",
    "mydat['HEPB'] = pd.get_dummies(mydat['HEPB'], drop_first = True)\n",
    "mydat['Sex'] = pd.get_dummies(mydat['Sex'], drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis and Visualization\n",
    "As the name suggests, pandas.corr() will compute pairwise correlation of (numerical) columns, excluding NA/null values. Notice that in this case, since we've converted 'HEPB' to a number (0 or 1) we can see how correlated different features are with the HEPB label!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate and bivariate description analysis \n",
    "hepb = sns.countplot(x='HEPB', data = mydat)\n",
    "hepb.set_xticklabels(labels=['Postive', 'Negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hepb = sns.countplot(x='Sex', data = mydat)\n",
    "hepb.set_xticklabels(labels=['Male', 'Female'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etplt = sns.countplot(x = 'Ethn', data = mydat)\n",
    "etplt.set_xticklabels(labels=('Mexican American', 'Other Hispanic', 'Non-Hispanic White', 'Non-Hispanic Black', 'Other Race'),rotation=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(mydat.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"HEPB\", hue=\"Sex\", data=mydat,\n",
    "                 kind=\"count\")\n",
    "g.set_xticklabels(labels=['Postive', 'Negative'])\n",
    "g.legend(title='Sex', loc='upper left', labels=['Male', 'Female'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydat.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin to implement a model, we need to prepare the variables that will be used. Let us see the levels in all categorical variables in order to create dummy\\indicator variables for each level. At this step you'll have to normalize and transform variables if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the levels name of all categorical variables \n",
    "for col in categorical_columns:\n",
    "    print(col, mydat[col].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating indicator/dummify for each level of your categorical variable\n",
    "mydat1 = pd.get_dummies(mydat, columns = [\"Ethn\", \"Sex\"])\n",
    "\n",
    "mydat1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(mydat1.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing age variable\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "norm_data = mydat1.Age.values.reshape(-1, 1)\n",
    "scaler.fit(norm_data)\n",
    "mydat1['nAge'] = scaler.transform(norm_data)\n",
    "\n",
    "#data_unitnorm\n",
    "#sns.distplot(mydat1['nAge'])\n",
    "\n",
    "mydat1 = mydat1.drop(['Age'], axis=1)\n",
    "mydat1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and selection\n",
    "Now, let's split our data into training and testing in an 80-20 split, stratified by HEPB distribution (this tries to keep the HEPB distribution approximately equal for the training and test set). For consistency, let's use a random seed 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val = train_test_split(mydat1, test_size = 0.20, random_state = 0, stratify = mydat['HEPB'])\n",
    "\n",
    "\n",
    "y_train = data_train['HEPB']\n",
    "y_val = data_val['HEPB']\n",
    "\n",
    "# only features \n",
    "X_train = data_train[[\"nAge\",\"Ethn_1\",\"Ethn_2\", \"Ethn_3\",\"Ethn_4\", \"Ethn_5\", \"Sex_0\" , \"Sex_1\"]]\n",
    "X_val = data_val[[\"nAge\",\"Ethn_1\",\"Ethn_2\", \"Ethn_3\",\"Ethn_4\", \"Ethn_5\", \"Sex_0\" , \"Sex_1\"]]\n",
    "\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "results = logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "# apply the model to test data\n",
    "y_val_predict = logreg.predict(X_val)\n",
    "y_val_proba = logreg.predict_proba(X_val)\n",
    "\n",
    "print(y_val[:5],y_val_predict[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Metrics\n",
    "## Confusion Matrix Metrics\n",
    "\n",
    "There are several useful metrics that are derived from the confusion matrix:\n",
    "\n",
    "![alt text](https://i.imgur.com/uipmEwt.png)\n",
    "\n",
    "* sensitivity, **recall**, hit rate, or true positive rate (TPR) : $ \\mathrm {TPR} ={\\frac {\\mathrm {TP} }{P}}={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FN} }}$\n",
    " \n",
    "* **precision** or positive predictive value (PPV) : $ \\mathrm {PPV} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FP} }}$\n",
    "\n",
    "* specificity or true negative rate (TNR) : $\\mathrm {TNR} ={\\frac {\\mathrm {TN} }{N}}={\\frac {\\mathrm {TN} }{\\mathrm {TN} +\\mathrm {FP} }}$\n",
    "\n",
    "* miss rate or false negative rate (FNR) : $ \\mathrm {FNR} ={\\frac {\\mathrm {FN} }{P}}={\\frac {\\mathrm {FN} }{\\mathrm {FN} +\\mathrm {TP} }}=1-\\mathrm {TPR}$\n",
    "\n",
    "* fall-out or false positive rate (FPR) : $\\mathrm {FPR} ={\\frac {\\mathrm {FP} }{N}}={\\frac {\\mathrm {FP} }{\\mathrm {FP} +\\mathrm {TN} }}=1-\\mathrm {TNR} $\n",
    "\n",
    "* accuracy (ACC) : $\\mathrm {ACC} ={\\frac {\\mathrm {TP} +\\mathrm {TN} }{P+N}}={\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {TP} +\\mathrm {TN} +\\mathrm {FP} +\\mathrm {FN} }}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# This is exctly the first metric you'll be evaluated on!\n",
    "# Note: this will only work on the binary case -- you'll need a different method to do multi-class case\n",
    "def cm_metric(y_true,y_prob):\n",
    "    \n",
    "    # predict the class with the greatest probability\n",
    "    y_pred = [np.argmax(y) for y in y_prob]\n",
    "\n",
    "    # calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_val_predict)\n",
    "\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    return sum(sum(np.multiply(cm_norm,np.array([[1, -2], [-2, 1]]))))\n",
    "\n",
    "cm_metric(y_val,y_val_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is classifying everything as class 1... Pretty terrible. :( Well maybe there's a threshold where this doesn't happen. Let's look at the AUC ROC.\n",
    "\n",
    "## AUC ROC\n",
    "\n",
    "A receiver operating characteristic (ROC) is a probability curve that plots the true positive rate (y) against the false positive rate (x) at many decision threshold settings. The area under the curve (AUC) represents a measure of separability or how much the model is capable of distinguishing between classes. An AUC closer to 1 is desirable as it shows the model is perfectly distinguishing between patients with disease and no disease. A poor model has an AUC $\\leq$ 0.50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the FPR and TPR at varying thresholds (assume label 1 is the \"postive\" class)\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_val, y_val_proba[:,1])\n",
    "\n",
    "# Calculate the area under the ROC curve\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "print('AUC: ',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot ROC\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model only looks at three possible features and leaves lots of room for improvement!  Try using more features, different models, and see if you can do anything about the data we threw out earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economic Cost\n",
    "Similar to the confusion matrix, we want you to keep in mind the other aspects of healthcare analytics--in this case, economic feasibility. In essence, we want you to minimize the amount of time and money spent on data collection by **reducing the number of features** collected. Each record certainly required a lot of time and money from several individuals and businesses to reliably create, and we hope you gain a better understanding of conducting a useful cost-benefit analysis with this scoring method.  This won't be evaluated quantitatively, but please consider discussing it for your presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your presentation on Friday, don't foget to mention why you selected the features you used, the model implemented, the scoring metrics mentioned above, make sure to have both results for test and training sets, and the limitations of your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "For those that finish early, try different classification models such as decision trees, KNN, SVM etc. You can try tackling the multiclass classifier (predicting the different cases instead of simply negative or positive)!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
